---
title: "MVAHomework2.Rmd"
author: "KDR"
date: "10/24/2019"
output: word_document
---

PROBLEM 1)

Use the bivariate boxplot on the scatterplot of pairs of variables ((temp, wind),
(temp, precip)) in the air pollution data to identify any outliers. Calculate the correlation between each pair of variables using all the data and the data with any
identified outliers removed. 
Comment on results. 
```{r}
#install.packages("MVA")
library(MVA)
data("USairpollution", package = "HSAUR2")
#USairpollution

# extract temp and wind variables 
ProblemOneData <- USairpollution[, c("temp", "wind")]
#ProblemOneData


# draw bivariate boxplot of the two variables; temp and wind.
bvbox(ProblemOneData, xlab = "City Temperature", ylab = "Wind Speed")

text(USairpollution$temp, USairpollution$wind, cex= 0.6, labels=
abbreviate(row.names(USairpollution)))
# This Box Plot shows two measurments outside the bounds of the the 99% confidence interval. 
```

This above bivariate boxplot allows the user to see Miami and Pheonix as the outlier cities 
corresponsing to the dataset's "temp" and "wind" variables. 


Now, let's repeat the process, but using "temp" and "precip" variables from the dataset. 
```{r}
# the extract temp and precip
ProblemOneDataB <- USairpollution[, c("temp", "precip")]
#ProblemOneDataB
bvbox(ProblemOneDataB, xlab = "City Temperature", ylab = "Precipitation Percentage")

text(USairpollution$temp, USairpollution$precip, cex= 0.6, labels=
abbreviate(row.names(USairpollution)))
```

This boxplot identifies Pheonix, Albuquerque, Miami, and Denver as outliers, as they fall outside
of the 99% confidence interval. 



The following method provides a cleaner visual, as other city names are eliminated, allowing the
viewer to easy spot the data's outliers. This same method can be used for the first dataset (ProblemOneData).
```{r, dpi =300}
lab <- c("Phoenix", "Albuquerque",
         "Miami", "Denver")

# match returns a vector of the positions of (first) matches of its first argument in its second. 
outcity <- match(lab, rownames(USairpollution))

# draw bivariate boxplot (must load MVA package)
bvbox(ProblemOneDataB, xlab = "Temperature", ylab = "Precipitation")

# lable the cities
text(ProblemOneDataB$temp[outcity], ProblemOneDataB$precip[outcity],
     labels = lab, cex = 0.8, pos = 1)
```
We see above that Denver, Albuquerque, Phoenix and Miami have been verified and is visually more
pleasing. 


Since outliers allow for bias (and bad statistics), we want to eliminate them from 
the data set prior to making final calulations. The difference with and without the 
outliers are shown below. 

The correlation of temperature and wind speed from the dataset:
```{r, dpi=300}

cor(ProblemOneData$temp, ProblemOneData$wind)


#three outlier cities matched with dataframes
outcity <- match(c("Albuquerque", "Phoenix", "Miami"), row.names(USairpollution))

# compute the correlations without outliers
cor(ProblemOneData$temp[-outcity], ProblemOneData$wind[-outcity])


# After deleting the outliers, we see the correlation changes from -0.3497396 to -0.2559154.

```
The correlation of temperature and precipitation from the dataset:
```{r, dpi=300}

cor(ProblemOneDataB$temp, ProblemOneDataB$precip)


#four outlier cities matched with dataframes
outcity <- match(c("Albuquerque", "Phoenix", "Miami", "Denver"), row.names(USairpollution))

# compute the correlations without outliers
cor(ProblemOneDataB$temp[-outcity], ProblemOneDataB$precip[-outcity])


# After deleting the outliers, we see the correlation change from 0.3862534 to 0.6572746

```

PROBLEM 2)

The banknote dataset contains measurements on 200 Swiss banknotes: 100 genuine and 100 counterfeit
The variables are the status of the "note", length of bill, width of left edge, width of right
edge, bottom margin width, and top margin width. All measurements are in millimeters. Read the data
and pick the variables: "note", "top_margin", and "diag_length".

A) Construct separate univariate kernel estimates (gaussian kernel) of the distribution of
these two variables. Experiment with bandwidths (actual numbers, not defaults) to get 
nice-looking graphs. 

Kernel density estimation for 'top margin' data:
```{r, dpi=300, fig.width=5, fig.height=4}

banknote <- read.csv("http://westfall.ba.ttu.edu/isqs6348/Rdata/swiss.csv")
banknoteData <- banknote[,c(1,6,7)]
#banknoteData
top_margin_data <- banknoteData[,c(2)]
#top_margin_data
plot(density(top_margin_data, bw= .2, kernel = "gaussian"))
rug(top_margin_data)
```

In the plot above, you'll notice roughness instead of a smooth bell-curve.
This was done intentionally, as I chose to see how the data varied and where 
the variables were more condensed. 





Kernel density estimation for 'diag length' data:
```{r, dpi=300, fig.width=5, fig.height=4}
diag_length_data <- banknoteData[,c(3)]
#diag_length_data
plot(density(diag_length_data, bw = .2, kernel = "gaussian"))
rug(diag_length_data)
```

Above, I chose a bandwidth of .2 in order to display the bimodality of the data. 




PROBLEM 2) 

B) (i). Using the bivariate Gaussian kernel, estimate the bivariate density of the two
variables using (i) a contour plot and (ii) a 3-D perspective plot. Use the bandwidths you
finally chose in part a. 

Contour Plot:
```{r, dpi-300, fig.width=5, fig.height=4}
library("KernSmooth")

density2D <- bkde2D(banknoteData[,2:3], bandwidth = .2)

plot(banknoteData$top_margin, banknoteData$diag_length, xlab = "top margin",
     ylab = "diag length",
     main = "Bank Note Measurements")
# add contours for bivariate densities
contour(x = density2D$x1, y = density2D$x2, z = density2D$fhat, add = TRUE)
```
PROBLEM 2)
B) (ii). 3-D Perspective Plot:
```{r, dpi=300, fig.width=5, fig.height=4}
persp(x = density2D$x1, y = density2D$x2,
      z = density2D$fhat,
      xlab = "top margin",
      ylab = "diag width",
      zlab = "density",
      main= "Perspective Plot of Bank Note's Top Margin and Diag Width", phi=50)
```
I set phi to 50 in order to see the variance in the data more accurately (from above).



PROBLEM 2) 

c) Plot the scatterplot, highlight points with different colors according to whether the bills
are real or fake (the"note" variable in the data set has that information). 
Explain your findings. 
```{r, dpi=300}
#banknote

#install.packages("ResourceSelection")
library(ResourceSelection)
plot(banknote[,2:3], col = banknote[,1])
label = levels(banknote[,1])
legend("topright", label, pch=1, col = 1:2)
```

PROBLEM 3) 

Examine the multivariate normality (MVN) of the pottery data (excluding the "kiln"
variable) by creating the chi-square plot of the data. Load the data as follow.
Follow the listed steps to examine the multivariate normality 

```{r}
#install.packages("HSAUR2")
library(MVA)
x <- pottery[,-10]
#x
```
PROBLEM 3)

A). Find the column-mean vector
```{r}
xbar <- colMeans(x)
xbar
```
PROBLEM 3)

B). Find the covariance matrix of the data.
```{r}
S <- cov(x)
S
```
PROBLEM 3)

C). Find the Mahalanobis distances given using the data, and the result of part a, 
and part b. 
```{r}
d2 <- mahalanobis(x, xbar, S)
d2
```
PROBLEM 3)

D) Sort the Mahalonibis distances from smallest to largest
```{r}
sort(d2, decreasing = FALSE)

```
PROBLEM 3)

E) (i). Chi-Square plot:
```{r}
quantiles <- qchisq((1:nrow(x) - 1/2) / nrow(x), df = ncol(x))
sd2 <- sort(d2)

plot(quantiles, sd2,
     xlab =expression(paste(chi[3]^2, "Quantile")),
     ylab = "Sorted Mahalonobis distances")
abline(a = 0, b = 1)
text(quantiles, sd2, abbreviate(names(sd2)), col = "red")
```

E) (ii). Multivariate Normally Distributed Random Numbers:
```{r}
# Simulation
set.seed(1234)
#install.packages("MASS")
library(MASS)
n = 1000
x <- mvrnorm(n, xbar, S)
plot(x, main = "Simulation Multivariate Data")

# Test multivariate normality
xbar <- colMeans(x)
S <- cov(x)
d2 <- mahalanobis(x, xbar, S)
quantiles <- qchisq((1:nrow(x)- 1/2) / nrow(x), df = ncol(x))
sd2 <- sort(d2)

plot(quantiles, sd2,
     xlab = expression(paste(chi[3]^2, "Quantile")),
     ylab = "Ordered squared distances")
abline(a = 0, b = 1)
```

Problem 3. 
F). Interpret the plot of part E.
E(i): This plot is not linear, although it resembles linearity. The top and bottom of the
line plot display outliers (shown by its curvature). 
E(ii):The chi-square plot of the data shows linearity in the data, with the exception of a few
outliers.

If we were to delete the outliers, we could display a near-linear plot and document the
statistics, as results would prove more accurate after this task. 


























